{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e05af2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f26275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('secrets/openai_api_key_old.txt', 'r') as f:\n",
    "    openai.api_key = f.read().strip()\n",
    "    \n",
    "# with open('secrets/openai_api_key.txt', 'r') as f:\n",
    "#     openai.api_key = f.read().strip()\n",
    "    \n",
    "system_prompt = '''\n",
    "You are an expert writer and commentator. \n",
    "The user will give you an article, and you will write a summary and a thoughtful opinion/analysis.\n",
    "\n",
    "The summary should be a paragraph long, contain key technical details, and be easy to understand. \n",
    "The summary should highlight key words and concepts from the article without abstracting them away. \n",
    "It should end with the key takeaway from the article.\n",
    "\n",
    "The opinion/analysis should provide insightful commentary based on information outside of what's strictly in the article. \n",
    "It should provide an interesting, sometimes critical take on the article's subject matter, and it should leave the reader some food for thought. \n",
    "This should short and to the point.\n",
    "\n",
    "Respond in markdown.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3362f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Introducing speech-to-text, text-to-speech, and more for 1,100+ languages\"\n",
    "subtitle = \"\"\n",
    "content = '''\n",
    "Equipping machines with the ability to recognize and produce speech can make information accessible to many more people, including those who rely entirely on voice to access information. However, producing good-quality machine learning models for these tasks requires large amounts of labeled data — in this case, many thousands of hours of audio, along with transcriptions. For most languages, this data simply does not exist. For example, existing speech recognition models only cover approximately 100 languages — a fraction of the 7,000+ known languages spoken on the planet. Even more concerning, nearly half of these languages are in danger of disappearing in our lifetime.\n",
    "\n",
    "\n",
    "\n",
    "Demonstration of our speech-to-text models for a few of the over 1,100 languages our model supports.\n",
    "RECOMMENDED READS\n",
    "\n",
    "Meta’s new AI-powered translation system for Hokkien pioneers new approach for unwritten languages\n",
    "MuAViC: The first audio-video speech translation benchmark\n",
    "Advancing direct speech-to-speech modeling with discrete units\n",
    "In the Massively Multilingual Speech (MMS) project, we overcome some of these challenges by combining wav2vec 2.0, our pioneering work in self-supervised learning, and a new dataset that provides labeled data for over 1,100 languages and unlabeled data for nearly 4,000 languages. Some of these, such as the Tatuyo language, have only a few hundred speakers, and for most of these languages, no prior speech technology exists. Our results show that the Massively Multilingual Speech models outperform existing models and cover 10 times as many languages. Meta is focused on multilinguality in general: For text, the NLLB project scaled multilingual translation to 200 languages, and the Massively Multilingual Speech project scales speech technology to many more languages.\n",
    "\n",
    "Today, we are publicly sharing our models and code so that others in the research community can build upon our work. Through this work, we hope to make a small contribution to preserve the incredible language diversity of the world.\n",
    "\n",
    "\n",
    "Illustration of the languages the Massively Multilingual Speech (MMS) recognition model supports. MMS supports speech-to-text and text-to-speech for 1,107 languages and language identification for over 4,000 languages.\n",
    "Our approach\n",
    "\n",
    "Collecting audio data for thousands of languages was our first challenge because the largest existing speech datasets cover at most 100 languages. To overcome it, we turned to religious texts, such as the Bible, that have been translated in many different languages and whose translations have been widely studied for text-based language translation research. These translations have publicly available audio recordings of people reading these texts in different languages. As part of this project, we created a dataset of readings of the New Testament in over 1,100 languages, which provided on average 32 hours of data per language.\n",
    "\n",
    "By considering unlabeled recordings of various other Christian religious readings, we increased the number of languages available to over 4,000. While this data is from a specific domain and is often read by male speakers, our analysis shows that our models perform equally well for male and female voices. And while the content of the audio recordings is religious, our analysis shows that this does not overly bias the model to produce more religious language. We believe this is because we use a Connectionist Temporal Classification approach, which is far more constrained compared with large language models (LLMs) or sequence to-sequence models for speech recognition.\n",
    "\n",
    "\n",
    "Analysis of potential gender bias. Automatic speech recognition models trained on Massively Multilingual Speech data have a similar error rate for male and female speakers on the FLEURS benchmark.\n",
    "We preprocessed the data to improve quality and to make it usable by our machine learning algorithms. To do so, we trained an alignment model on existing data in over 100 languages and used this model together with an efficient forced alignment algorithm that can process very long recordings of about 20 minutes or more. We applied multiple rounds of this process and performed a final cross-validation filtering step based on model accuracy to remove potentially misaligned data. To enable other researchers to create new speech datasets, we added the alignment algorithm to PyTorch and released the alignment model.\n",
    "\n",
    "Thirty-two hours of data per language is not enough to train conventional supervised speech recognition models. This is why we built on wav2vec 2.0, our prior work on self-supervised speech representation learning, which greatly reduced the amount of labeled data needed to train good systems. Concretely, we trained self-supervised models on about 500,000 hours of speech data in over 1,400 languages — this is nearly five times more languages than any known prior work. The resulting models were then fine-tuned for a specific speech task, such as multilingual speech recognition or language identification.\n",
    "\n",
    "\n",
    "Results\n",
    "\n",
    "To get a better understanding of how well models trained on the Massively Multilingual Speech data perform, we evaluated them on existing benchmark datasets, such as FLEURS.\n",
    "\n",
    "We trained multilingual speech recognition models on over 1,100 languages using a 1B parameter wav2vec 2.0 model. As the number of languages increases, performance does decrease, but only very slightly: Moving from 61 to 1,107 languages increases the character error rate by only about 0.4 percent but increases the language coverage by over 18 times.\n",
    "\n",
    "\n",
    "Error rate on 61 FLEURS languages for multilingual speech recognition systems trained on Massively Multilingual Speech data when increasing the number of languages supported by each system from 61 to 1,107. Higher error rates indicate lower performance.\n",
    "In a like-for-like comparison with OpenAI’s Whisper, we found that models trained on the Massively Multilingual Speech data achieve half the word error rate, but Massively Multilingual Speech covers 11 times more languages. This demonstrates that our model can perform very well compared with the best current speech models.\n",
    "\n",
    "\n",
    "Word error rate of OpenAI Whisper compared to Massively Multilingual Speech on the 54 FLEURS languages that enable a direct comparison.\n",
    "Next, we trained a language identification (LID) model for over 4,000 languages using our datasets as well as existing datasets, such as FLEURS and CommonVoice, and evaluated it on the FLEURS LID task. It turns out that supporting 40 times the number languages still results in very good performance.\n",
    "\n",
    "\n",
    "Language identification accuracy on the VoxLingua-107 benchmark of existing work, supporting just over 100 languages, and MMS, which supports over 4,000 languages.\n",
    "We also built text-to-speech systems for over 1,100 languages. Current text-to-speech models are typically trained on speech corpora that contain only a single speaker. A limitation of the Massively Multilingual Speech data is that it contains relatively few different speakers for many languages, and often only a single speaker. However, this is an advantage for building text-to-speech systems, and so we trained such systems for over 1,100 languages. We found that the speech produced by these systems is of good quality, as the examples below show.\n",
    "\n",
    "\n",
    "Demonstration of the MMS text-to-speech models synthesizing text in Yoruba, Iloko and Maithili.\n",
    "\n",
    "We are encouraged by our results, but as with all new AI technologies, our models aren’t perfect. For example, there is some risk that the speech-to-text model may mistranscribe select words or phrases. Depending on the output, this could result in offensive and/or inaccurate language. We continue to believe that collaboration across the AI community is critical to the responsible development of AI technologies.\n",
    "\n",
    "Toward a single speech model supporting thousands of languages\n",
    "\n",
    "\n",
    "Many of the world’s languages are in danger of disappearing, and the limitations of current speech recognition and speech generation technology will only accelerate this trend. We envision a world where technology has the opposite effect, encouraging people to keep their languages alive since they can access information and use technology by speaking in their preferred language.\n",
    "\n",
    "The Massively Multilingual Speech project presents a significant step forward in this direction. In the future, we want to increase the language coverage to support even more languages, and also tackle the challenge of handling dialects, which is often difficult for existing speech technology. Our goal is to make it easier for people to access information and to use devices in their preferred language. There are also many concrete use cases for speech technology — such as VR/AR technology — which can be used in a person’s preferred language - to messaging services that can understand everyone’s voice.\n",
    "\n",
    "We also envision a future where a single model can solve several speech tasks for all languages. While we trained separate models for speech recognition, speech synthesis, and language identification, we believe that in the future, a single model will be able to accomplish all these tasks and more, leading to better overall performance.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71d1e17a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Summary:**\n",
       "\n",
       "Meta has introduced the Massively Multilingual Speech (MMS) project, which combines wav2vec 2.0 and a new dataset to provide speech-to-text, text-to-speech, and language identification for over 1,100 languages. The project aims to make information more accessible and help preserve language diversity. The MMS models outperform existing models and cover ten times as many languages. The dataset was created using readings of the New Testament in over 1,100 languages, providing an average of 32 hours of data per language. The models were trained on 500,000 hours of speech data in over 1,400 languages, and the results show good performance compared to current speech models.\n",
       "\n",
       "**Opinion/Analysis:**\n",
       "\n",
       "The MMS project is a significant step forward in making technology more inclusive and accessible to a broader range of people. By supporting over 1,100 languages, it has the potential to help preserve endangered languages and promote cultural diversity. However, it's essential to consider the limitations of the dataset, which is primarily based on religious texts and may not be representative of everyday language use. Additionally, the focus on male speakers in the dataset could lead to potential biases in the models. Despite these limitations, the MMS project is a promising development in the field of speech recognition and synthesis, and its potential applications in VR/AR technology and messaging services could greatly benefit users worldwide."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_prompt = f'''\n",
    "Title: {title}\n",
    "Subtitle: {subtitle}\n",
    "{content}\n",
    "'''\n",
    "\n",
    "messages = [\n",
    "    {'role': 'system', 'content': system_prompt},\n",
    "    {'role': 'user', 'content': user_prompt}\n",
    "]\n",
    "\n",
    "summary = openai.ChatCompletion.create(\n",
    "    model='gpt-4', \n",
    "    messages=messages,\n",
    "    max_tokens=500,\n",
    "    temperature=0\n",
    ").choices[0]['message']['content']\n",
    "\n",
    "display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa5b2ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jacky",
   "language": "python",
   "name": "jacky"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
