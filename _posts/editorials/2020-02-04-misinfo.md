---
title: "[Misinformation Editorial Title WIP]"
author: [janny_zhang, jacky_liang]
categories: [editorials]
tags: [hype, misinfo, DeepFakes]
excerpt: "[Excerpt WIP]"
image: 
  feature: assets/img/editorials/2020-02-04-misinfo/main.jpg
  credit: <a href="https://unbabel.com/blog/artificial-intelligence-fake-news/">Unbabel</a>
permalink: /editorials/misinfo
---

## WHAT YOU NEED TO KNOW

*   Misinformation (misinfo) is "incorrect or misleading information," and disinformation (disinfo) is a subset of that which is "deliberately and often covertly spread… in order to influence public opinion or obscure the truth."
*   Because it's optimized to catch people's attention, misinfo spreads more quickly than true information; the Internet allows this process to speed up even further. As misinfo spreads, it erodes trust in communications, media, and society, thus posing great dangers to democracies. Every year, more countries are found running disinfo campaigns online, with 70 countries found doing so in 2019.
*   Though many people fear sophisticated artificial intelligence (AI) and machine learning's (ML) potential role in exacerbating the spread of misinfo, bad actors are not using cutting-edge technologies to spread misinfo. Instead, they prefer to pay people to write false content and to exploit the AI/ML algorithms we use everyday in order to reach their target audiences, relying on their readers' knee-jerk reactions to spread their messages further.
*   Dozens of organizations are already taking various approaches to fight the spread of misinfo and actively build a stronger information ecosystem. On an individual level, it's best to develop stronger critical thinking and mindful media consumption habits to avoid worsening the spread of misinformation online.

## INTRODUCTION

In the United States, the term "fake news" has lost its bite through overuse since the 2016 presidential election. 
Though misinformation (misinfo) is not a new phenomenon, its impact and spread is particularly severe in the last decade as trolls, malicious actors, and state-sponsored media groups exploit the new affordances of social media and the Internet. Everyone--authority figures, journalists, everyday citizens--is a potential target. 
In their [2020 Doomsday Clock Statement](https://thebulletin.org/doomsday-clock/current-time/), the Bulletin of the Atomic Scientists states that the greatest dangers to humanity "are compounded by a threat multiplier, cyber-enabled information warfare, that undercuts society's ability to respond."

Artificial intelligence (AI) can--and does--play a nontrivial role in the spread of misinformation. 
When [Pew Research canvassed 979 scientific experts on artificial intelligence](https://www.pewinternet.org/2018/12/10/artificial-intelligence-and-the-future-of-humans/) in 2018, one common theme was that AI could allow increased "mayhem" or "further erosion of traditional sociopolitical structures and the possibility of great loss of lives," partially due to "the use of weaponized information, lies and propaganda to dangerously destabilize human groups." 

Despite global consequences, online misinformation continues to live in a cloud of relative obscurity, and AI's role in exacerbating the problem is particularly misunderstood. 
With this article, I hope to help readers consider the state and severity of online misinformation, as well as understand AI's impact.

## DEFINITIONS

To talk about AI's potential role in the spread of misinfo online, we need a solid understanding of what "misinformation" and its child "disinformation" mean.

**[Misinformation](https://www.merriam-webster.com/dictionary/misinformation)** (misinfo) is "incorrect or misleading information." 
Misinfo is not necessarily misleading on purpose; it could be the result of improper communication, incomplete information on the source's part, or any number of reasons.

**[Disinformation](https://www.merriam-webster.com/dictionary/disinformation)** (disinfo) is a subset of misinfo.
It is "false information deliberately and often covertly spread (as by the planting of rumors) in order to influence public opinion or obscure the truth." 
**[False](https://www.merriam-webster.com/dictionary/false)** means "not genuine; intentionally untrue; adjusted or made so as to deceive; intended or tending to mislead"; in other words, disinfo can be any mix of fabricated information and true information taken out of context.

## WHY WE SHOULD CARE

News outlets or your friends and family may have informed you that "fake news" is vaguely dangerous. 
But why, exactly, is it important to learn about disinfo campaigns and the spread of misinfo online? And to what extent is it affecting you?

### **EXPOSURE TO MISINFO IS UP; PROTECTIONS AGAINST IT ARE DOWN.**

As of November 2018, television is still the most popular source of news for most Americans -- but [a nearly equal number of Americans now get news online](https://www.journalism.org/2019/03/26/nearly-as-many-americans-prefer-to-get-their-local-news-online-as-prefer-the-tv-set/). 
And at least on social media, misinformation spreads [further and deeper](https://science.sciencemag.org/content/359/6380/1146.full?utm_source=sciencemagazine&utm_medium=reddit&utm_campaign=vosoughi-18311) than true information.

<figure class="half">
    <img src="{{ site.imgpath }}/editorials/2020-02-04-misinfo/im0.png"/>
    <figcaption>
        Misinformation spreads further and deeper than true information on Twitter
        <a href="https://science.sciencemag.org/content/359/6380/1146.full?utm_source=sciencemagazine&utm_medium=reddit&utm_campaign=vosoughi-18311">[Vosoughi et al]</a>.
        Original caption:
        "Complementary cumulative distribution functions (CCDFs) of true and false rumor cascades. (A) Depth. (B) Size. (C) Maximum breadth. (D) Structural virality. (E and F) The number of minutes it takes for true and false rumor cascades to reach any (E) depth and (F) number of unique Twitter users. (G) The number of unique Twitter users reached at every depth and (H) the mean breadth of true and false rumor cascades at every depth. In (H), plot is lognormal. Standard errors were clustered at         the rumor level (i.e., cascades belonging to the same rumor were clustered together)"
    </figcaption>
</figure>

Some groups of people may be more susceptible to [online fake news](http://www.dartmouth.edu/~nyhan/fake-news-2016.pdf) than others. Seniors (age 65+) don't go online as much as younger folks, but ["\[once they\] join the online world, digital technology often becomes an integral part of their daily lives"](https://www.pewinternet.org/2014/04/03/older-adults-and-technology-use/) and they're not prepared to navigate the online infosphere.
[^ft0]

<figure class="half">
    <img src="{{ site.imgpath }}/editorials/2020-02-04-misinfo/im1.png"/>
    <figcaption>
        "Younger Americans are better able to distinguish between factual and opinion news statements" <a href="https://www.pewresearch.org/fact-tank/2018/10/23/younger-americans-are-better-than-older-americans-at-telling-factual-news-statements-from-opinions/">[Pew Research]</a>.
    </figcaption>
</figure>

However, people raised in the digital era are not experts at navigating the online infosphere either: [most students in middle school, high school, and college do not have the ability to identify and be skeptical of sponsored content, uncited images, or advocacy-organization-backed tweets](https://stacks.stanford.edu/file/druid:fv751yt5934/SHEG%20Evaluating%20Information%20Online.pdf). 
Furthermore, many young Americans (ages 18-29) get their [news online](https://www.pewresearch.org/fact-tank/2016/10/06/younger-adults-more-likely-than-their-elders-to-prefer-reading-news/ft_16_09_30_newsreaders/), so they are more likely to be exposed to content by unknown and unvetted info sources. 
[^ft1]

Even people primed to be skeptical of varying news sources are having a difficult time figuring out the truth. 
In a [2017 Gallup poll](https://news.gallup.com/poll/226157/americans-struggle-navigate-modern-media-landscape.aspx), though Americans are more skeptical of news sources than ever, 58% of polled Americans found it "harder to be well-informed," and less Americans believed that there were "enough sources to sort out facts."

<figure class="half">
    <img src="{{ site.imgpath }}/editorials/2020-02-04-misinfo/im2.png"/>
    <figcaption>
        "Americans Increasingly Say Sorting Out Fact From Bias Is Difficult" <a href="https://news.gallup.com/poll/226157/americans-struggle-navigate-modern-media-landscape.aspx">[Gallup]</a>
    </figcaption>
</figure>

### **MISINFO LOWERS OVERALL TRUST IN THE NEWS (IN AMERICA).**

Misinfo not only impacts how people approach something, but (if uncovered) fundamentally shakes people's faith in the news ([Wired](https://www.wired.com/story/free-speech-issue-tech-turmoil-new-censorship/)). 
According to [Pew Research](https://www.journalism.org/2019/06/05/many-americans-say-made-up-news-is-a-critical-problem-that-needs-to-be-fixed/), "nearly seven-in-ten U.S. adults (68%) say made-up news and information greatly impacts Americans' confidence in government institutions, and roughly half (54%) say it is having a major impact on our confidence in each other."

<figure class="half">
    <img src="{{ site.imgpath }}/editorials/2020-02-04-misinfo/im3.png"/>
    <figcaption>
        "Americans see made-up news as a bigger problem than other key issues, and most see it as detrimental to the country's democratic system." <a href="https://www.journalism.org/2019/06/05/many-americans-say-made-up-news-is-a-critical-problem-that-needs-to-be-fixed/">[Pew Research]</a>
    </figcaption>
</figure>

### **MISINFO IS AFFECTING PEOPLE GLOBALLY.**

Looking globally, the effects of misinformation may be amplified in places either newer to the digital infosphere or highly nationalist. 
India is both: [BBC reports](https://www.bbc.co.uk/news/resources/idt-e5043092-f7f0-42e9-9848-5274ac896e6d) that at least 24 people in India were killed during 2018 "in incidents involving rumours spread on social media or messaging apps," primarily WhatsApp, where many "false rumours warn[ed] people that there [were] child abductors in their towns, driving locals to target innocent men [unknown] to the community." 
[^ft2]

On a larger scale, [University of Oxford researchers report that](https://comprop.oii.ox.ac.uk/wp-content/uploads/sites/93/2019/09/CyberTroop-Report19.pdf) in "26 countries, computational propaganda is being used as a tool of information control in three distinct ways: to suppress fundamental human rights, discredit political opponents, and drown out dissenting opinions." 
Furthermore, there is evidence of "organized social media manipulation campaigns which have taken place in 70 countries."

<figure class="half">
    <img src="{{ site.imgpath }}/editorials/2020-02-04-misinfo/im4.png"/>
    <figcaption>
        There has been a 150% "increase in countries using organised social media manipulation campaigns over the last two years." 
        <a href="https://comprop.oii.ox.ac.uk/wp-content/uploads/sites/93/2019/09/CyberTroop-Report19.pdf">[University of Oxford]</a>
    </figcaption>
</figure>

### **MISINFO HAS MEASURABLE CONSEQUENCES ACROSS FIELDS.**

The Internet makes it easier to spread and access information. This characteristic means misinformation online has an amplified impact on many things, which I've provided examples for below.

#### Natural Disasters

Citizen information and misinfo both [helped and harmed in significant ways](https://www.mei.edu/publications/information-filtering-social-media-during-disasters) during Hurricane Sandy, the ebola outbreaks panic, and the Japan tsunami of 2011. 
In recent events, celebrities and folks drumming up support for the 2019 Amazon fires accidentally shared calls to action on social media alongside pictures of Californian wildfires, and furthermore exaggerated [the severity of the situation with regards to its forecasted impact on the environment](https://www.forbes.com/sites/michaelshellenberger/2019/08/26/why-everything-they-say-about-the-amazon-including-that-its-the-lungs-of-the-world-is-wrong/#7bb9d7005bde).

<figure class="half">
    <img src="{{ site.imgpath }}/editorials/2020-02-04-misinfo/im5.png"/>
    <figcaption>
        Original caption: "While the number of fires in 2019 is indeed 80% higher than in 2018, it's just 7% higher than the average over the last 10 years ago" 
        <a href="https://www.forbes.com/sites/michaelshellenberger/2019/08/26/why-everything-they-say-about-the-amazon-including-that-its-the-lungs-of-the-world-is-wrong/#783dbb495bde">[Forbes]</a>
    </figcaption>
</figure>

#### Human Rights

Misinfo "has the potential to trigger new violence and abuses in fragile communities and exaggerate existing tensions" ([Koettl](https://www.repository.cam.ac.uk/bitstream/handle/1810/253508/Koettl_Citizen%20Media%20Research%20and%20Verifcation_FINAL%20%281%29.pdf?sequence=1&isAllowed=y)). 
Take the 2019 Hong Kong protests, which were escalated by China's [state-run disinformation campaign](https://www.wired.com/story/china-twitter-facebook-hong-kong-protests-disinformation/). 
Or the long-going and painful Israeli-Palestinian conflict, where [UN members stated in 2018](https://www.un.org/press/en/2018/pal2226.doc.htm) "that the media can be a part of the solution as much as the problem," and that "journalism [at least around this conflict] appears at times locked into irreconcilable narratives, hurting its own credibility."

#### Politics

Just pre-2016, clickbait websites based in [Macedonia](http://www.bbc.com/future/story/20190528-i-was-a-macedonian-fake-news-writer) and [San Francisco](https://www.theguardian.com/technology/2016/aug/24/facebook-clickbait-political-news-sites-us-election-trump) were created to mislead people about the 2016 U.S. presidential elections.
Regular citizens learned how to gain ideological traction online by coordinating with like-minded individuals to make [digital political movements take up more attention online](https://www.politico.com/magazine/story/2017/08/09/twitter-trump-train-maga-echo-chamber-215470). 
The Institute for the Future released eight case studies on social and issue-oriented groups in the U.S. conducted during the 2018 midterm elections, which illustrated how [disinfo campaigns attempt to shift public policy by targeting and splintering vulnerable groups](http://www.iftf.org/fileadmin/user_upload/downloads/ourwork/IFTF_Executive_Summary_comp.prop_W_05.07.19_01.pdf). 
When the [Alabama](https://www.cjr.org/the_media_today/alabama_abortion_ban_law.php) and Georgia abortion laws were debated online mid-2019, there was outrage on social media where liberals and conservatives alike misinterpreted the laws in context and responded with either [panic or misinformed support](https://www.buzzfeednews.com/article/janelytvynenko/alabama-abortion-ban-misinformation).

<figure class="half">
    <img src="{{ site.imgpath }}/editorials/2020-02-04-misinfo/im6.png"/>
    <figcaption>
        Original caption: "A screenshot from the 'Trumps WarRoom' group message on Twitter." 
        <a href="https://www.politico.com/magazine/story/2017/08/09/twitter-trump-train-maga-echo-chamber-215470">[Politico Magazine]</a>
    </figcaption>
</figure>

As these machinations play out, American youth's trust in institutions are significantly lower than their parents' and grandparents' ([Pew](https://www.pewresearch.org/fact-tank/2019/08/06/young-americans-are-less-trusting-of-other-people-and-key-institutions-than-their-elders/)).

#### Science, Technology, + Development

Funnily enough, there is [plenty of misinformation](http://approximatelycorrect.com/2017/03/28/the-ai-misinformation-epidemic/) about [technologies](http://approximatelycorrect.com/2018/06/05/ai-ml-ai-swirling-nomenclature-slurried-thought/) which are capable of propagating misinfo, and people who are afraid of the consequences of new tech lobby to shut down their development. 
[Anti-vaccine sentiments](https://www.nature.com/articles/d41586-018-07034-4) spread as concerned citizens continue to believe the (already retracted) single work purportedly showing a tie between "autism and the measles, mumps and rubella (MMR) vaccine," which has served to bring measles back from the brink of extinction and line the pockets of the original author. 
People continue to fight against basic scientific consensuses, partially because thinkers from the George C Marshall Institute (a major conservative think tank, now integrated into the CO<sub>2</sub> Coalition) back in the 1980's launched a [concentrated, economically-motivated campaign](https://books.google.com/books?hl=en&lr=&id=ZeFcCAAAQBAJ&oi=fnd&pg=PA1&ots=jbCpovcMg-&sig=hqp-P8H3ie7zPfjTyZEMXtEPRhQ#v=onepage&q&f=false) against "cigarettes being harmful to humans" and  "climate change being real." 
[^ft3]
And these only scrape the surface of what's out there.

## HOW IT WORKS

The state of misinformation affects outcomes in the real world. So how does it work, and how is it exploited?

### **MISINFO APPEARS IN MANY FORMS.**

There are a few different kinds of form factors that misinformation can appear in. 
Paraphrasing this [article](https://www.cjr.org/tow_center/6_types_election_fake_news.php) by the Columbia Journalism Review (2016), we have the following type breakdown (ordered loosely by increasing intent to mislead):

1. **Parody content**, which created for the purpose of satire but can mislead consumers if they do not understand the creator's intent.
2. **Authentic material used in the wrong context**, which could have been put in the wrong context for any number of reasons, but ends up misinforming consumers.
3. **Fake information**," which could have been a result of the creator misunderstanding the information they accessed and reporting it improperly, the creator themselves having been exposed to misinformation and reporting it in good faith, or the creator deliberately making false information (see example tweet below).
4. **Manipulated photos, videos, or audio**, which is then used as an artifact to support an otherwise baseless argument.
5. **Imposter news sites**, designed to look like brands we already know, which usually try to take advantage of brand recognition to get more traffic to trick people to visit their own site.
6. **Fake news sites**, which are committed to creating news that supports a particular agenda, though they often also post "real" content on non-divisive issues to increase their own credibility (ex. RT, previously Russia Today, and Sputnik, known among specialists as the propaganda wing of the Russian government).

<figure class="half">
    <img src="{{ site.imgpath }}/editorials/2020-02-04-misinfo/im7.png"/>
    <figcaption>
        A tweet sharing a fabricated digital poster which falsely claims that "Hillary voters get to vote via text". 
        <a href="https://www.cjr.org/tow_center/6_types_election_fake_news.php">[Columbia Journalism Review]</a>
    </figcaption>
</figure>

Certainly, types of disinformation are not easy to distinguish from each other--to its own benefit. 
As a [Myanmar disinformation veteran](https://www.nytimes.com/2018/10/15/technology/myanmar-facebook-genocide.html) notes, there is "a golden rule for false news: If one quarter of the content is true, that helps make the rest of it believable."

### **MISINFO IS CREATED AND SPREAD FOR VARIOUS REASONS.**

Misinformation can be created and spread accidentally or purposefully. 
When purposeful, it can either be for auxiliary gains or for directly manipulating people via the infosphere. 
[^ft4]

One notable case of disinformation for **financial gain** is referenced in this [evaluation of anti-vaxxer misinformation](https://www.nature.com/articles/d41586-018-07034-4), noting "the canonical example… the 1998 publication by infamous former physician Andrew Wakefield purporting to show a link between autism and the measles, mumps and rubella (MMR) vaccine." 
Wakefield's [work on the autism-MMR link has been long discredited](https://www.independent.co.uk/news/health/andrew-wakefield-who-is-mmr-doctor-anti-vaccine-anti-vaxxer-us-a8328326.html) by the research community. 
Originally, one might think that he and his co-authors clunkily misinterpreted the information they had and reported their results without due diligence, but [later findings](https://www.bmj.com/content/342/bmj.c5258) report that Wakefield stood to profit significantly from fighting against the MMR vaccine. 
Still, the study's damage was done: parents today continue to fear vaccinating their children, which allowed [measles to reappear in the U.S.](https://www.cdc.gov/mmwr/volumes/66/wr/mm6627a1.htm).

Other disinformation can be created for **political gain**. 
Mid-2019, the New York Times reported on one of President Trump's top media-creators, Patrick Mauldin, who [created a quartet of look-alike political campaign sites](https://www.nytimes.com/2019/06/29/us/politics/fake-joe-biden-website.html) for the top four Democratic candidates for the 2020 presidential race. 
Each website portrayed the candidates as racists, hypocrites, or ruthless, and labelled themselves as "a political parody built and paid for 'BY AN American citizen FOR American citizens,' and not the work of any campaign or political action committee." These sites successfully deceived many citizens into believing they were the original campaign sites, distorting their opinions on the candidates. 
When asked for his motivations, Mr. Mauldin said he was "only trying to deliver hard truths. 'I mean, [Democrats] could [see their candidates for who they were] themselves… But they're not. That's the problem.'"

Disinformation can also be carried out for **ideological purposes**. 
One educational and timely example of this can be found in the Russian Internet Research Agency (IRA)'s tactics to sow polarization in the United States. 
The [official report by the then-Special Counsel Robert S. Mueller III of the U.S. Department of Justice](https://intelligence.house.gov/social-media-content/) goes into specific details on how the Russian strategy has played out since 2014 in order to "further a broader Kremlin objective: sowing discord in the U.S. by inflaming passions on a range of divisive issues... by weaving together fake accounts, pages, and communities to push politicized content and videos, and to mobilize real Americans to sign online petitions and join rallies and protests." 
The effects of these initiatives can be seen by the numbers: on Facebook alone, there were "more than 11.4 million American users exposed to [Russian-sponsored] advertisements; 470 IRA-created Facebook pages; 80,000 pieces of organic content created by those pages; and [e]xposure of organic content to more than 126 million Americans." 
Politico [predicts that the 2020 U.S. presidential elections will see similar disinformation campaigns](https://www.politico.com/news/2019/12/01/fight-against-disinformation-2020-election-074422) from within America.

### **MISINFO SPREADS VIA MANUFACTURED SOCIAL MEDIA ACCOUNTS.**

Disinformation is most easily amplified via fake social media accounts. These come in two forms:

1. **[Bot accounts](https://www.theatlantic.com/technology/archive/2019/05/how-spot-irans-fake-news-disinformation-campaign/589438/)**, which are made to share links, or other blog posts. 
   [^ft5]    
2. **Human-managed fake social media accounts**, often organized into troll farms. 
   [^ft6]    

A few big roles that fake accounts play in disinfo campaigns online are:

1. **Falsely amplifying viewpoints online**.
   [^ft7]   
2. **Coordinating to harass and take down opposing views**.
   [^ft8]   

<figure class="half">
    <img src="{{ site.imgpath }}/editorials/2020-02-04-misinfo/im8_9_10.png"/>
    <figcaption>
        Posts from an artist describing how their old artwork was removed for "hate speech" after they reposted it with a pro-Hong-Kong message, then their experience with being shadowbanned on Instagram soon after. 
        <a href="https://www.facebook.com/YuumeiArt/posts/6-days-ago-i-made-a-post-on-instagram-in-support-of-freedom-for-hong-kong-this-i/2525545027494256/">[Yuumei, on Facebook]</a>
    </figcaption>
</figure>

Twitter has found [accounts from countries all over the world pushing state agendas over the last three years](https://about.twitter.com/en_us/values/elections-integrity.html#data), including Iran, Bangladesh, Venezuela, Catalonia, China, Saudi Arabia, Ecuador, the United Arab Emirates, and Spain. 
The Myanmar military spread [anti-Rohingya](https://www.nytimes.com/2018/10/15/technology/myanmar-facebook-genocide.html) messages on Facebook, using "its rich history of psychological warfare that it developed during the decades when Myanmar was controlled by a military junta... to discredit radio broadcasts from the BBC and Voice of America." 
[^ft9]

<figure class="half">
    <img src="{{ site.imgpath }}/editorials/2020-02-04-misinfo/im11.png"/>
    <figcaption>
        Original caption: "The photos claim to show evidence of conflict in Myanmar's Rakhine State in the 1940s, but the images are from Bangladesh's war for independence from Pakistan in 1971." 
        <a href="https://www.nytimes.com/2018/10/15/technology/myanmar-facebook-genocide.html">[New York Times]</a>
    </figcaption>
</figure>

Some methods used by disinfo campaigns, which are similar to those used by trolls online, are:

1. **Ruining the opposing side's reputation and community** by purposefully pretending to be an extremist on that side. 
   [^ft10]
   
2. **Wasting opponents' time** or trying to ruin their image "by pretending to ask sincere questions, but… feigning ignorance and repeating 'polite' follow ups until someone gets fed up [in order to] cast their opponents as attacking them and being unreasonable", commonly known as [sealioning](https://www.forbes.com/sites/marshallshepherd/2019/03/07/sealioning-is-a-common-trolling-tactic-on-social-media-what-is-it/#52e905fa7a41). 
   [^ft11]
   
Disinformation serves to introduce mayhem and erode trust in communications, media, and society. 
As such, it is a particularly dangerous weapon against democracies.

## AI + TECH'S POTENTIAL ROLE IN MISINFO

Readers may have noted that most of the techniques mentioned in _HOW IT WORKS_ require significant human management and involvement. 
Thus, despite [plenty of fear and noise](https://www.theguardian.com/technology/2018/jul/25/ai-artificial-intelligence-social-media-bots-wrong) blurring the dialogue, it's important to remember that the technologies used in creating and spreading misinfo should not be as much a focus as the people and organizations using them.

Still, there's no denying that AI & tech have enabled these behaviors on an unprecedented scale. 
In hopes of demystifying the technologies present in creating and spreading misinfo, we present a survey of technologies that are, or can be, used for misinfo.

### **TECH + AI WORSENING DISINFO CAMPAIGNS**

For this section, we've broken our technologies into those which have already been used for disinformation and those which could feasibly be exploited in upcoming years.

#### NOW

In practice, most disinformation campaigns use groups of people to write and share biased and false content to manipulate public opinion. 
[Oxford researchers found that](https://comprop.oii.ox.ac.uk/wp-content/uploads/sites/93/2019/09/CyberTroop-Report19.pdf) 87% of countries use fake human accounts, 80% of countries use bot accounts ("highly automated accounts designed to mimic human behaviour online") and 11% use cyborg accounts ("which blend automation with human curation") to spread disinformation.

There's still a lot of human interference in generating misinfo. 
[^ft12]
However, it is already possible for exploiters to use AI to generate misinformative content; there are informational step-by-step articles that [break down how to create your own AI-populated troll farms](https://www.shellypalmer.com/2018/03/build-troll-farm/) and [make deepfake videos](https://www.alanzucconi.com/2018/03/14/create-perfect-deepfakes/). 
The resources and knowledge necessary for small scale campaigning is easy to obtain, and a great number of disinfo campaigns could not have occurred without bot farms or AI-generated content.

##### _Targeting individuals_

People used deepfakes to make fake sex videos of Rana [Ayyub](https://www.ted.com/talks/danielle_citron_how_deepfakes_undermine_truth_and_threaten_democracy/transcript), a journalist in India who covers sensitive topics such as human rights violations, to [ruin her reputation, distract her from her work, and spur the local community to harass her](https://www.huffingtonpost.in/rana-ayyub/i-was-the-victim-of-a-deepfake-porn-plot-intended-to-silence-me-says-rana-ayyub_a_23595592/). 
[^ft13]

##### _Targeting communities_

Twitter has found [accounts linked to governments](https://about.twitter.com/en_us/values/elections-integrity.html#data) and their campaigns, such as [China's tweets](https://blog.twitter.com/en_us/topics/company/2019/information_operations_directed_at_Hong_Kong.html) pushing back against the Hong Kong protests. 
By using their supporters and [offering enticing wages](https://www.nytimes.com/2015/06/07/magazine/the-agency.html), governments can gather large groups who are willing to write misleading articles, posts, and comments online under various pseudonyms. 
Other than supporting their own group's ideals, bot farms also aim to irritate opponents to [negatively affect the opponents' reputations online](http://www.iftf.org/fileadmin/user_upload/downloads/ourwork/IFTF_Executive_Summary_comp.prop_W_05.07.19_01.pdf).

Many groups use bot farms to generate hate against one another, like when these [Turkish accounts](https://medium.com/dfrlab/bot-like-turkish-accounts-wage-anti-kurdish-hashtag-campaign-9b1a2908f5b3) popularized the hashtag #BabyKillerPKK to mobilize people against the Kurdish people. 
[^ft14]

##### _Media Platforms_

Bot farms have also been used to sow discord around anti-misinfo efforts, such as the [information panel](https://support.google.com/youtube/answer/9004474?hl=en) that YouTube introduced in late 2018 to combat misinformation around conspiracy-theory hot topics. 
Most of the users in [this thread discussing how to remove the info panel](https://support.google.com/youtube/thread/948226?hl=en) are low-effort bot accounts; if you click the usernames and look at their activity histories, you find that most use generated usernames a la "User \<number here\>" or "\<name\>\<number\>", or that they have only been used once or twice to respond to this thread.

#### FUTURE

There are many potential technologies that could be used for misinfo/disinfo in the future: deepfake apps, article generators, and so on. 
Most sophisticated tech is usually not worth the intellectual investment for bad actors, but it doesn't hurt to know what to look out for. We'll go into what those are and how they work below.

##### _Deepfakes_

FakeApp.org appears to have been the primary source for personal deepfake generation, but currently it seems to be down (albeit possible to find with targeted searches). 
Now, it seems that people recommend [DeepFaceLab](https://github.com/iperov/DeepFaceLab) for making deepfakes; there are tutorials for using both on YouTube. 
These and similar technologies are the most accessible for non-academics, since they require the least amount of tuning and technological savvy. 
Most advanced technologies require a significant amount of domain knowledge and resources to set up and train properly, which creates a steep wall for entry.

However, this barrier to entry is being lowered by the day. 
Off-the-shelf tools enabled a tech journalist to create his own deepfake video in [two weeks with about $500](https://arstechnica.com/science/2019/12/how-i-created-a-deepfake-of-mark-zuckerberg-and-star-treks-data/). 
Dating apps are looking to use AI-generated faces to make [their user base appear more diverse](https://www.washingtonpost.com/technology/2020/01/07/dating-apps-need-women-advertisers-need-diversity-ai-companies-offer-solution-fake-people/). 
Facebook recently announced how [they shut down a disinformation campaign network](https://www.nytimes.com/2019/12/20/business/facebook-ai-generated-profiles.html) of over "610 Facebook accounts, 89 Facebook Pages, 156 Groups and 72 Instagram accounts" that used AI-generated faces as profile pictures. 
The [Face Swapping GAN (FSGAN](https://nirkin.com/fsgan/) creates non-trivially real-looking deepfakes without extensive training on the particular faces/actions the user wants to blend. Technology is making it easier for deep-fakers to create better results.

<figure class="half">
    <img src="{{ site.imgpath }}/editorials/2020-02-04-misinfo/im12.png"/>
    <figcaption>
        A sample of how FSGAN takes one person's face and puts it on another person's body. <a href="https://nirkin.com/fsgan/">[Nirkin et al]</a>
    </figcaption>
</figure>

##### _Text generation_

In the academic sphere, OpenAI's language model [GPT-2](https://openai.com/blog/better-language-models/) is named in many places as tech that can possibly be used to generate entire fake text articles. They recently released the [most trained version](https://openai.com/blog/gpt-2-1-5b-release/), which you can [demo online here](https://talktotransformer.com/), but report that they've "seen no strong evidence of misuse so far."

<figure class="half">
    <img src="{{ site.imgpath }}/editorials/2020-02-04-misinfo/im13.png"/>
    <figcaption>
        A sample of GPT-2's generated article text, based on a human-written prompt. <a href="(https://openai.com/blog/better-language-models/">[OpenAI]</a>
    </figcaption>
</figure>

Another cutting-edge model for generating misinfo is [GROVER]( https://arxiv.org/pdf/1905.12616.pdf), which, given any headline, can generate a full article. 
In testing, "humans find these generations to be more trustworthy than human-written disinformation." 
Thankfully, content-generation technologies are far from perfect.
Most reviews of GPT-2 generated text say it doesn't hold up well to [critical inspection](https://nostalgebraist.tumblr.com/post/187579086034/it-seems-pretty-clear-to-me-by-now-that-gpt-2-is).

One of WIRED's AI & tech policy staff writers, Gregory Barber, rightly points our attention to the fact that most generated fakes are still relatively [easy to identify](https://www.wired.com/story/deepfakes-getting-better-theyre-easy-spot/). 
Still, Barber reminds us experts like law professor Robert Chesney say that "political disruption doesn't require cutting-edge technology: it can result from lower-quality stuff, intended to sow discord, but not necessarily to fool." 
Since many people just read headlines and skim the rest of the article, GPT-2's level of sophistication is sufficient for general-purpose deception. 
If readers don't pay close, critical attention to the content they consume--which they often don't--imperfectly generated content could still [easily influence their minds](https://www.skynettoday.com/editorials/humans-not-concentrating) on important issues. 
[^ft15]

That said, though fancy new technologies and AI may seem foreboding, they don't really bring anything new to the table other than allowing scammers and bad actors to streamline their processes and reach a wider audience -- and even then, said scammers rarely use them.

### **TECH + AI WORSENING THE EFFECTS OF UNINTENTIONAL MISINFORMATION**

Bad actors simply don't need to invest in sophisticated technologies to deceive their intended audiences. All they need to do is take advantage of everyday technologies. 
For this section, we look only at previously or currently available tech and AI.

##### _Social media optimizations_

AI algorithms such as [multi-arm bandits](https://arxiv.org/pdf/1904.10040.pdf) and [various machine learning models](https://www.forbes.com/sites/quora/2017/05/15/your-social-media-news-feed-and-the-algorithms-that-drive-it/#2b6a00b14eb8) play a key role in keeping us returning to social media and scrolling mindlessly by trying to optimize for consumer engagement. 
The biggest central social media platforms online are all the products of companies, whose survival depends primarily on revenue, much of which is generated from ads. See their business forms in the following expanding section. 
[^ft16]

Often times, the recommended content are either mindlessly entertaining or highly polarizing which elicits strong reactions, which are easily made by bad actors. 
Both types of content can lower people's vigilance and allow them to fall prey to misinformation.

##### _Active exploitations of seemingly neutral technologies_

Google's search engine is widely viewed as neutral, but even their services have been exploited to spread disinformation. 
Google has [contextual knowledge panels](https://www.theatlantic.com/technology/archive/2019/09/googles-knowledge-panels-are-magnifying-disinformation/598474/), which serve as "a collection of definitive-seeming information… that appears when you Google someone or something famous," which appear next to their query results. 
While these knowledge panels are monitored, they are [populated automatically](https://support.google.com/knowledgepanel/answer/9163198?hl=en) from partner sites, which [bad actors can exploit](https://www.theatlantic.com/technology/archive/2019/09/googles-knowledge-panels-are-magnifying-disinformation/598474/) to amplify false information.

<figure class="half">
    <img src="{{ site.imgpath }}/editorials/2020-02-04-misinfo/im14.png"/>
    <figcaption>
        A contextual knowledge panel exploited to falsely list Nazism as one of the ideologies of the California Republican Party 
        <a href="https://www.vice.com/en_us/article/vbq38d/google-is-listing-nazism-as-the-first-ideology-of-the-california-republican-party">[Vice]</a>
    </figcaption>
</figure>

Another way to take advantage of Google search is through [search engine optimization](https://www.lexico.com/definition/search_engine_optimization) (SEO), or the "process of maximizing the number of visitors to a particular website by ensuring that the site appears high on the list of results returned by a search engine." 
This is done by many companies for marketing purposes, but has become a co-opted troll tactic; one such example of troll SEO is when [thousands of Reddit users upvoted a picture of President Trump](https://www.theguardian.com/us-news/2018/jul/17/trump-idiot-google-images-search) which was captioned with the word "idiot" until Google's query results for "idiot" returned his image first. 
One major vulnerability to ill-intentioned SEO is [the data void](https://datasociety.net/output/data-voids/), which consists of "search engine queries that turn up little to no results" which can be easily taken over and populated primarily "by manipulators eager to expose people to problematic content." 

##### _Old news-consumption methods in a new world_

Social media usage is [increasing around the world](https://datareportal.com/reports/digital-2019-global-digital-overview), and many communities new to the infosphere do not have the digital literacy infrastructure or social norms in place to prevent them from believing much of what they see online. BBC's 2018 study on the spread of misinformation in [India, Kenya, and Nigeria](https://www.bbc.com/news/world-46146877) found that participants:

> made little attempt to query the original source of fake news messages, looking instead to alternative signs that the information was reliable. 
These included the number of comments on a Facebook post, the kinds of images on the posts, or the sender, with people assuming WhatsApp messages from family and friends could be trusted and sent on without checking."

Technology has been a great equalizer in terms of lowering the barrier to entry for people to easily create and spread information, but that has also shifted the information landscape in ways that consumers haven't adjusted to.

### **MITIGATING THE EFFECTS OF MISINFO**

Thankfully, organizations of all types and sizes are aware of and working to counter misinfo's effects. 
This is an active area of research, but we'll provide a sweeping survey of these efforts -- including non-technological initiatives. Many of the cited organizations provide their own pros and cons for their efforts, or those discussions can be found easily online, so we won't go into them here.

#### COMPANIES

The creators of some of the biggest technologies which have amplified misinfo online, such as Facebook (which also owns WhatsApp), Twitter, and Google (which also owns YouTube), are taking various steps to combat "coordinated inauthentic behavior" on their platforms. 
[^ft17]

##### _Preemptively identifying and informing users of problematic content_

Existing [fact-check](https://www.axios.com/facebook-fact-checking-contractors-e1eaeb8b-54cd-4519-8671-d81121ef1740.html) and [content-moderation](https://www.theverge.com/2019/2/25/18229714/cognizant-facebook-content-moderator-interviews-trauma-working-conditions-arizona) pipelines such as [those used at Facebook](https://www.facebook.com/help/1952307158131536) currently rely primarily on human workers, but companies are expanding automated fact-checking and content moderation using machine learning models. 
Tattle is a startup that connects WhatsApp users to special fact-checking channels, in which ML can ["automatically categorize those tips and distribute relevant fact checks."](https://www.poynter.org/fact-checking/2019/these-projects-are-using-ai-to-fight-misinformation/).
Facebook has made efforts to use ML to moderate content in various ways, such as by [flagging videos that contain violent, mayhem-inducing content](https://www.technologyreview.com/f/614774/this-is-how-facebooks-ai-looks-for-bad-stuff/). 
Google introduced [information panels](https://support.google.com/youtube/answer/9004474) mid-2018 on YouTube videos covering "a small number of well-established historical and scientific topics that have often been subject to misinformation," automatically linking to sources that strive to give users further context on those videos.

##### _Combating false amplification of ideas on their platforms_

In a 2019 effort to minimize propaganda and misinfo on its platform, Twitter banned [all political](https://twitter.com/jack/status/1189634360472829952) and other advertisements from [state-controlled media](https://blog.twitter.com/en_us/topics/company/2019/advertising_policies_on_state_media.html). 
Mid-2017, [Facebook reported](https://fbnewsroomus.files.wordpress.com/2017/04/facebook-and-information-operations-v1.pdf) its work to "recognize inauthentic accounts" engaged in false amplification by looking at "patterns of activity" instead of the content that they share--which is related to networks research that has been done in media studies for years. 
Taking a similar approach, Twitter [acquired the machine-learning startup Fabula AI](https://blog.twitter.com/en_us/topics/company/2019/Twitter-acquires-Fabula-AI.html) mid-2019 to strengthen its efforts in detecting network manipulation. 
Leading cyber-security firm FireEye trained a GPT-2 model to [detect Russian IRA-style texts](https://www.fireeye.com/blog/threat-research/2019/11/combatting-social-media-information-operations-neural-language-models.html) by first training a generative model on IRA propaganda, then training a separate detection model to detect both generated and original IRA text. 
In late 2019, [Twitter also added a feature which allows users to hide replies to tweets](https://twitter.com/Twitter/status/1197551185894559744?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1197551185894559744&ref_url=https%3A%2F%2Fwww.cnn.com%2F2019%2F11%2F21%2Ftech%2Ftwitter-hide-replies%2Findex.html) to allow original posters to moderate the discussion threads, leaving the hidden replies accessible via a secondary page off the main thread.

##### _Funding news initiatives and shaping the news ecosystem_

Mid-2017, Facebook co-funded the CUNY Graduate School of Journalism's [News Integrity Initiative](https://www.journalism.cuny.edu/2017/04/announcing-the-new-integrity-initiative/), which is working with universities and journalism centers to create "a global consortium focused on helping people make informed judgments about the news they read and share online" 
[^ft18]
Google started the [Google News Initiative](https://newsinitiative.withgoogle.com/) in 2018, which aims "to help journalism thrive in the digital age" (primarily by building on the news ecosystem and creating fact-checking tools), as well as co-started the non-profit [First Draft](https://firstdraftnews.org/about/), which "supports journalists, academics and technologists working to address challenges relating to trust and truth in the digital age." 
In late 2019, Twitter collaborated with Adobe and the New York Times to start the [Content Authenticity Initiative](https://news.adobe.com/press-release/corporate/adobe-new-york-times-company-and-twitter-announce-content-authenticity), to "provide proper content attribution for creators and publishers" and "provide consumers with an attribution trail to give them greater confidence about the authenticity of the content they're consuming."

To aid deepfake detection research, [Google released a large dataset](https://ai.googleblog.com/2019/09/contributing-data-to-deepfake-detection.html) of videos altered by deepfake technology and their unaltered counterparts. 
The Partnership on AI, an organization with members including Amazon, Facebook, and Microsoft, also hosts a [Deepfake Detection Challenge](https://deepfakedetectionchallenge.ai/).

<figure class="half">
    <img src="{{ site.imgpath }}/editorials/2020-02-04-misinfo/im15.png"/>
    <figcaption>
        Original caption: "A sample of videos from Google's contribution to the FaceForensics benchmark. 
        To generate these, pairs of actors were selected randomly and deep neural networks swapped the face of one actor onto the head of another." 
        <a href="https://ai.googleblog.com/2019/09/contributing-data-to-deepfake-detection.html">[Google AI Blog]</a>
    </figcaption>
</figure>

As organizations on the front lines of technologies that are used for misinformation, companies often have the greatest knowledge of patterns of misinformative behavior and the best datasets for training.

#### ACADEMIA

Academics from all disciplines are studying misinformation online. 
[^ft23]
Their works helps us understand the scope of the problem.

##### _Identifying generated content_

The researchers behind both [GROVER]( https://arxiv.org/pdf/1905.12616.pdf) and [GPT-2](https://openai.com/blog/better-language-models/) (the text-generation models we mentioned before) have trained and tried detection models on their technologies, with GROVER's detection model reporting 92% accuracy and GPT-2's reporting 95% accuracy on successfully detecting its own generated texts as non-human. 
[^ft19]
Other researchers from Adobe and UC Berkeley worked together to train an AI model that can [detect if an image has been warped by Photoshop](https://www.fxguide.com/fxfeatured/adobe-photoshop-ai-detector/).
The Photoshop research also looks into how to "undo" warping, to help reconstruct original images.

<figure class="half">
    <img src="{{ site.imgpath }}/editorials/2020-02-04-misinfo/im16.png"/>
    <figcaption>
        A graphic showing how images are processed to detect warping <a href="https://www.fxguide.com/fxfeatured/adobe-photoshop-ai-detector/">[fxguide]</a>
    </figcaption>
</figure>

##### _Automating fact-checking_

Researchers at the University of Texas at Arlington released [ClaimBuster](https://idir.uta.edu/claimbuster/), a real-time automated fact checking tool which detects assertions of facts from transcripts (for example, of presidential debates) and forwards statements that merit fact-checking to journalists. 
The website also has an "end-to-end fact-checking" tool that tries to match input statements to ones that have already been fact-checked, effectively allowing users to search its database. 
Students at UC Berkeley released [SurfSafe](https://www.dailycal.org/2018/09/05/uc-berkeley-students-launch-browser-extension-to-combat-fake-photos/), "a free browser extension… that detects fake or doctored photos" by checking them against a database of known manipulated images.

##### _Embracing interdisciplinarity_

Misinformation is an old, complex and interdisciplinary problem, and technology is far from the only way to attack the problem. 
Since the way people look at situations affects how they craft remedies and solutions, researchers with differing backgrounds bring new potential solutions.
To foster collaboration, several misinformation conferences have been organized, such as [MisinfoCon](https://misinfocon.com/) (MIT), the upcoming [Information Ethics Roundtable](https://philevents.org/event/show/77250) focusing on "Scientific Misinformation in the Digital Age" (Northeastern University), and the upcoming [International Research Conference](https://waset.org/fake-news-social-media-manipulation-and-misinformation-conference-in-july-2020-in-rome) focusing on "Fake News, Social Media Manipulation and Misinformation" (various universities around the world) -- just to name a few. 
[^ft20]

Interdisciplinary researchers approach misinformation in markedly different and thereby valuable ways. 
For example, [Danah Boyd](https://www.danah.org/) (founder of [Data and Society](https://datasociety.net/)), [Claire Wardle](https://cyber.harvard.edu/people/dr-claire-wardle) (co-founder of First Draft) and David G. Rand (associate professor of Management Science and Brain and Cognitive Sciences at MIT Sloan) focus on situations such as [how YouTube's algorithms lead to hyper-radicalization](https://points.datasociety.net/media-manipulation-strategic-amplification-and-responsible-journalism-95f4d611f462), [how BBC decides whether to trust a new plethora of first-hand information, or "user-generated content"](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.477.1410&rep=rep1&type=pdf), and [how social networks constrain the free flow of information](http://ide.mit.edu/sites/default/files/publications/2019%20Stewart%20et%20al%20-%20Nature.pdf), refining the problem's scope in the process.
Academia brings a strong big-picture perspective to the field.

#### NEWS OUTLETS

News outlets that care about news integrity remain potential grounding forces in the information ecosystem. Some notable examples of the counter-misinfo efforts I've seen are provided below.

##### _Giving users context_

Context is important for any news story, but the chronological context of a headline is often lost when it is shared through social media as users (unintentionally) and publishers (intentionally) recycle outdated articles.
The trouble with this practice is two-fold: [sensational old news often becomes fake news as newer information debunk old claims](https://centerforinquiry.org/blog/why-old-news-is-often-fake-news/), and sharing the same story many times gives the impression that the event it describes happens more often than it does.

To combat this issue, publishers like [The Guardian have begun drawing attention to their article's ages](https://www.bbc.com/news/technology-47799878) with small but notable design changes:
<figure class="half">
    <img src="{{ site.imgpath }}/editorials/2020-02-04-misinfo/im17.png"/>
    <figcaption>
        Original caption: "Our new signposting shows the age of older articles" 
        <a href="https://www.theguardian.com/help/insideguardian/2019/apr/02/why-were-making-the-age-of-our-journalism-clearer">[Guardian]</a>
    </figcaption>
</figure>

They also add the year of publication when the article is posted on social media:
<figure class="half">
    <img src="{{ site.imgpath }}/editorials/2020-02-04-misinfo/im18.png"/>
    <figcaption>
        Original caption: "The age of a publication will now be visible on social media before you click on it" 
        <a href="https://www.theguardian.com/help/insideguardian/2019/apr/02/why-were-making-the-age-of-our-journalism-clearer">[Guardian]</a>
    </figcaption>
</figure>

##### _Communicating complex ideas effectively_

The New York Times has been particularly innovative in developing new ways to easily and accurately communicate ideas to their users via remarkable data visualizations (these have consistently won Malofiej Awards, known as the Pulitzers for infographics). 
Their data editor Amanda Cox is [well-known for adding to the Times' communication ability](https://www.nytimes.com/2019/02/28/reader-center/data-visualization-editor-amanda-cox.html), as they prioritize bringing to light more information "in ambitious enterprise stories, investigations as well as everyday news stories — that would be hidden but for creative investigation of data."

<figure class="half">
    <img src="{{ site.imgpath }}/editorials/2020-02-04-misinfo/im19.png"/>
    <figcaption>
        "Hell On Earth" infographic by the New York Times, gold medalist at Malofiej 27. 
        Picture from <a href="https://www.malofiejgraphics.com/2019/05/awards/national-geographic-and-reuters-win-endesa-best-of-show-award/">[Malofiej]</a>. 
        Interactive version available at the <a href="https://www.nytimes.com/interactive/2018/11/18/us/california-camp-fire-paradise.html">[New York Times]</a>.
    </figcaption>
</figure>

#### COMMUNITY

As companies, academia and news outlets contribute to mitigating the spread of misinformation from a top-down perspective, communities are able to work from the bottom up. 
The number of fact-checking organizations around the world [continues to grow](https://www.poynter.org/fact-checking/2019/number-of-fact-checking-outlets-surges-to-188-in-more-than-60-countries/), and there are many ongoing initiatives to teach digital literacy specifically in the context of determining misinformation. 
One such initiative is the [News Literacy Project](https://www.washingtonpost.com/education/2019/11/19/fighting-misinformation-pandemic-heres-help-teaching-students-distinguish-real-news-whats-fake/), a national educational nonprofit founded by a Pulitzer prize-winning reporter.
The project offers "[nonpartisan, independent programs](https://newslit.org/about/) that teach students how to know what to trust in the digital age".

Besides grassroots organizations, [governments around the world have begun taken legislative action against misinformation](https://www.poynter.org/ifcn/anti-misinformation-actions/) (though those efforts bring up very [important arguments about free speech and censorship](https://www.ft.com/content/b1d78fc2-57b4-11e9-a3db-1fe89bedc16e)).

---

The Online News Association compiled a large list of open initiatives in 2017, which can be found [here](https://medium.com/@ferg/heres-a-list-of-initiatives-that-hope-to-fix-trust-in-journalism-and-tackle-fake-news-30689feb402).
It includes collaborations and coalitions working on fact checking and verification, guides on how to handle content online, initiatives and studies on restoring trust, and related events and funding opportunities. 
[^ft21]
Addressing misinfo is a matter of not only hitting the problem head on but also creating an ecosystem in which high-quality information are shared, and people are diligently doing so.

## CONCLUSION

Modern information warfare is a sophisticated problem, and artificial intelligence only makes up one part of the phenomenon. 
It's important to think of "technology as an [amplifier](https://www.theatlantic.com/technology/archive/2011/03/technology-is-not-the-answer/73065/) of human intention." 
Technology itself won't be nefarious unless the users or designers had those intentions in the first place. 
As such, mitigating misinformation is a highly interdisciplinary task and requires fearless, creative thinking.

So what can we do? 
Brookings (a prestigious American public policy think tank) [summarizes it well](https://www.brookings.edu/research/how-to-combat-fake-news-and-disinformation/):

> 1. Governments should promote news literacy and strong professional journalism in their societies.
  2. The news industry must provide high-quality journalism in order to build public trust and correct fake news and disinformation without legitimizing them.
  3. Technology companies should invest in tools that identify fake news, reduce financial incentives for those who profit from disinformation, and improve online accountability.
  4. Educational institutions should make informing people about news literacy a high priority.
  5. Finally, individuals should follow a diversity of news sources, and be skeptical of what they read and watch.
   [^ft22]

There is still a future for trustworthy and healthy information, but there is much work that needs to be done.

## DEEPER DIVES

As a final note: while I've done my best to provide an introductory look into the state of disinformation with this article, there are natural limitations to the information I can offer, and I hope readers are patient with the gaps in my knowledge. 
I'd like to encourage interested readers to go forth, knowing how muddled the field can be, to look up more information yourself with sufficient skepticism and humility.

These are a number of papers/topics/resources that I recommend, and a fair number of them were referenced in this article:

*   Oxford University's report: [The Global Disinformation Order: 2019 Global Inventory of Organised Social Media Manipulation](https://comprop.oii.ox.ac.uk/wp-content/uploads/sites/93/2019/09/CyberTroop-Report19.pdf)
*   Michael Golebiewski and danah boyd's concept of [Data Voids: Where Missing Data Can Easily Be Exploited](https://datasociety.net/output/data-voids/)
*   Harvard's case study on [How the Chinese Government Fabricates Social Media Posts for Strategic Distraction, not Engaged Argument](http://gking.harvard.edu/files/gking/files/50c.pdf?m=1463587807), which provides a fascinating look at one way that disinformation can be used
*   BBC's research on differing worldwide responses to, and contexts for, misinformation in [India, Kenya, Nigeria](https://www.bbc.com/news/world-46146877)
*   The book [Merchants of Doubt: How a Handful of Scientists Obscured the Truth on Issues from Tobacco Smoke to Global Warming](https://books.google.com/books?hl=en&lr=&id=ZeFcCAAAQBAJ&oi=fnd&pg=PA1&ots=jbCpovcMg-&sig=hqp-P8H3ie7zPfjTyZEMXtEPRhQ#v=onepage&q&f=false)
*   Christoph Koettl's [Citizen Media Research and Verification: An Analytical Framework for Human Rights Practitioners](https://www.repository.cam.ac.uk/bitstream/handle/1810/253508/Koettl_Citizen%20Media%20Research%20and%20Verifcation_FINAL%20%281%29.pdf?sequence=1&isAllowed=y), which brings up interesting case studies for the importance of verifying "citizen media" as well as strategies for doing so.

[^ft0]: Their [digital literacy](https://www.edweek.org/ew/articles/2016/11/09/what-is-digital-literacy.html), or the "ability to use information and communication technologies to find, evaluate, create, and communicate information," is often underdeveloped, which makes them easier targets of disinfo campaigns and [7x more likely to share misinfo than younger folks](https://advances.sciencemag.org/content/5/1/eaau4586). 

[^ft1]: Even if younger Americans are usually better at telling [opinion from fact](https://www.pewresearch.org/fact-tank/2018/10/23/younger-americans-are-better-than-older-americans-at-telling-factual-news-statements-from-opinions/) than the elderly, repeated exposure to misleading headlines can [semi-permanently affect their first impressions](https://pdfs.semanticscholar.org/49a2/fdbabee0476ba0a69c4d8417ba5077d71a72.pdf) of various topics during a crucial age for developing opinions. 

[^ft2]: Furthermore in India, [people often forego fact-checking in favor of promoting content that support nationalist attitudes](http://downloads.bbc.co.uk/mediacentre/duty-identity-credibility.pdf).

[^ft3]: When the Chinese deep fake app [ZAO](https://thespinoff.co.nz/business/05-09-2019/deepfakes-face-swaps-and-the-future-of-identity-why-the-zao-app-went-viral/?fbclid=IwAR0VnVrF_DljYMYeQHkiSSeO8HTv94sUf9nALMk8KoQuEc8OP4ul09Po4BQ) attracted the attention of the Western world, Westerners proceeded to panic over the implications of the technology and [whether or not ZAO was a front](https://www.wired.co.uk/article/zao-app-download-deepfake) for the Chinese government to gather more data about users, though the truth is that China has already rolled out significantly more invasive [forms of surveillance](https://www.nytimes.com/2018/07/08/business/china-surveillance-technology.html) (which varies by [region](https://www.wired.com/story/inside-chinas-massive-surveillance-operation/)). Russia was found to spread [anti-GMO](https://osf.io/preprints/socarxiv/26ubf/) disinformation, possibly to drive more buyers towards its own "ecologically safe" agricultural products. 

[^ft4]: Confirming the purpose behind disinformation can be very difficult (especially since perpetrators rarely confess their motivations) and remains an open-ended task today. 

[^ft5]: In 2018, Pew Internet researchers found that [66% of tweeted links are shared by bot-like accounts](https://www.pewinternet.org/2018/04/09/bots-in-the-twittersphere/). 

[^ft6]: In 2015, the New York Times [reported on the Russian IRA](https://www.nytimes.com/2015/06/07/magazine/the-agency.html)'s method of paying people to work disinfo content creation jobs, which required "two 12-hour days in a row, followed by two days off. Over those two shifts [workers] had to meet a quota of five political posts, 10 nonpolitical posts and 150 to 200 comments on other workers' posts" for "41,000 rubles a month ($777)." 

[^ft7]: Twitter has released huge datasets of [state-backed throw-away Twitter accounts](https://about.twitter.com/en_us/values/elections-integrity.html#data) which have helped share bits of information, which distorts users' impressions of how popular those ideas are. 

[^ft8]: Paid state actors from China coordinated to exploit in-platform reporting mechanisms to [take down opposing views on Instagram](https://www.facebook.com/YuumeiArt/posts/6-days-ago-i-made-a-post-on-instagram-in-support-of-freedom-for-hong-kong-this-i/2525545027494256/).

[^ft9]: As mentioned earlier, Oxford has a more [detailed report on the tactics that countries](https://comprop.oii.ox.ac.uk/wp-content/uploads/sites/93/2019/09/CyberTroop-Report19.pdf) around the world have been using for computational warfare.

[^ft10]: In 2017, a number of [white supremacists created fake accounts pretending to be African Americans](https://www.npr.org/sections/codeswitch/2017/03/21/520522240/the-emergence-of-the-white-troll-behind-a-black-face) to take "revenge on Twitter" for banning some white supremacist and anti-Semitic ads, as well as to "[cause] blacks to panic." 

[^ft11]: Amy Johnson, a researcher at the Berkman-Klein Center for Internet and Society at Harvard University, goes into the long-lasting effects of sealioning in an essay in _[Perspectives of Harmful Speech Online](https://cyber.harvard.edu/sites/cyber.harvard.edu/files/2017-08_harmfulspeech.pdf)_. 

[^ft12]: For reference, read more about [Russia](https://www.fpri.org/article/2017/10/extremist-content-russian-disinformation-online-working-tech-find-solutions/)'s disinformation strategies which have spanned decades in their planning and execution, detailed more formally [here](https://www.justice.gov/file/1035477/download) and [analyzed here](https://dgap.org/system/files/article_pdfs/meister_isolationpropoganda_apr16_web_1.pdf) by political scientist Stefan Meister. 

[^ft13]: Similarly, this tactic has similarly been used on various [celebrities](https://www.bbc.com/news/technology-42912529), mostly female, to create pornographic content for "consumer enjoyment," which can affect these celebrities' reputations or wellbeing as well. Notable celebrities whose likenesses have been exploited towards this end are Gal Gadot, Emma Watson, Seolhyun, and Natalie Portman.

[^ft14]: Their primary tactic was to group the Kurdish People's Protection Units (YPG), which is not a terrorist group, with the Kurdistan Worker's Party (PKK), which is publicly acknowledged by many countries as such. 

[^ft15]: For example, a researcher trained a GPT-2 model to generate fake comments which were [submitted to a federal public comments website](https://techscience.org/a/2019121801/#Authors), and human readers were incapable of telling if a comment is genuine or fake better than random chance (you can try looking at these comments yourself [here](https://harvard.az1.qualtrics.com/jfe/form/SV_09gBWNLbx6IOmq1)).

[^ft16]: *   [Facebook + Instagram](https://www.sec.gov/Archives/edgar/data/1326801/000132680119000009/fb-12312018x10k.htm)'s Form 10-K, 2018: "The loss of marketers, or reduction in spending by marketers, could seriously harm our business. Substantially all of our revenue is currently generated from third parties advertising on Facebook and Instagram."
    *   [Twitter's](http://d18rn0p25nwr6d.cloudfront.net/CIK-0001418091/b353de9c-2664-4e81-8acb-52e9031e4836.pdf) Form 10-K, 2018: "The substantial majority of our revenue is currently generated from third parties advertising on Twitter. We generated approximately 86% of our revenue from advertising in each of the years ended December 31, 2017 and 2018."
    *   [Google's](https://www.sec.gov/Archives/edgar/data/1652044/000165204419000004/goog10-kq42018.htm) Form 10-K, 2018: "We generated over 85% of total revenues from advertising in 2018."

[^ft17]: Here are a number of articles and YouTube videos going into some of the things each specific platform does to combat misinfo: [Facebook](https://www.youtube.com/watch?v=FY_NtO7SIrY), [WhatsApp](https://www.wsj.com/articles/whatsapp-adds-tip-line-to-fight-misinformation-in-india-11554200672), [Twitter](https://www.youtube.com/watch?v=V-1RhQ1uuQ4), [Google](https://www.nytimes.com/2018/03/20/business/media/google-false-news.html), and [YouTube](https://www.youtube.com/watch?v=1PGm8LslEb4). 

[^ft18]: though the initiative has [since refocused](https://www.journalism.cuny.edu/centers/tow-knight-center-entrepreneurial-journalism/news-integrity-initiative/) on "improving diversity, equity, and inclusion (DEI) practices in the news business"

[^ft19]: Of course, both groups are wary of relying on those detection models to determine generated content from other models.

[^ft20]: One such conference, and some of its attendees: in 2017, Harvard and Northeastern University came together to host a conference on "Combating Fake News," where they decided on "An Agenda for Research and Action," [publishing its takeaways](https://www.sipotra.it/wp-content/uploads/2017/06/Combating-Fake-News.pdf). This was sponsored by Harvard's Shorenstein Center on Media, Politics and Public Policy and their Ash Center for Democratic Governance and Innovation; Northeastern University's NULab for Texts, Maps, and Networks and their Network Science Institute; featuring presentations from folks at institutions and companies such as MIT, BBC, the Council on Foreign Relations, UpWorthy, FactCheck.org, Microsoft Research, and Twitter -- truly a mix of academia, industry, technology, policy, information science, and other fields. 

[^ft21]: Some of these initiatives are mentioned in this article already.

[^ft22]: One way of starting to do so is looking into the News Literacy Project, which has many tips about common fallacies in news such as [This apple is not an orange! And other false equivalences](https://newslit.org/get-smart/news-lit-tip-false-equivalence/). I'd personally add that once individuals have educated themselves about the state of misinformation, they should share that knowledge with their own communities. Also, those particularly interested in mitigating misinformation should join or support the above groups.

[^ft23]: We've already cited many universities researching the state of misinformation, such as Oxford ([2019 Global Misinformation Order](https://comprop.oii.ox.ac.uk/wp-content/uploads/sites/93/2019/09/CyberTroop-Report19.pdf)), University of Cambridge ([Citizen Media Research and Verification](https://www.repository.cam.ac.uk/bitstream/handle/1810/253508/Koettl_Citizen%20Media%20Research%20and%20Verifcation_FINAL%20%281%29.pdf?sequence=1&isAllowed=y)), and Stanford ([Evaluating Information](https://stacks.stanford.edu/file/druid:fv751yt5934/SHEG%20Evaluating%20Information%20Online.pdf)). 